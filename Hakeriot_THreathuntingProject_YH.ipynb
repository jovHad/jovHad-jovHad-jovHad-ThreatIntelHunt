{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:29.304358Z",
     "start_time": "2025-02-09T00:19:29.298678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Making sure all the data is printed:\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ],
   "id": "9cbafe7339182203",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:29.316079Z",
     "start_time": "2025-02-09T00:19:29.312564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Global dictionary for protocol mappings\n",
    "PROTOCOL_MAPPING = {\n",
    "    1: 'ICMP',  # Internet Control Message Protocol\n",
    "    6: 'TCP',  # Transmission Control Protocol\n",
    "    17: 'UDP',  # User Datagram Protocol\n",
    "    50: 'ESP',  # Encapsulating Security Payload\n",
    "    51: 'AH',  # Authentication Header\n",
    "    8: 'EGP',  # Exterior Gateway Protocol\n",
    "    # Add other protocol mappings as needed\n",
    "}\n",
    "\n",
    "# Global Value to import the \"dapt2020\" files:\n",
    "CSV_DIR = \"/Users/admin/PycharmProjects/pythonProject3/hakeriot/mine\""
   ],
   "id": "f89474e5f6fa4d38",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:29.320962Z",
     "start_time": "2025-02-09T00:19:29.317821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_protocol_name(protocol_number):\n",
    "    # Function to get the protocol name\n",
    "    \"\"\"Return the human-readable name for a given protocol number.\"\"\"\n",
    "    return PROTOCOL_MAPPING.get(protocol_number, 'Other')"
   ],
   "id": "30bb25f2f048ed9f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:29.326200Z",
     "start_time": "2025-02-09T00:19:29.322665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def categorize_ports(port):\n",
    "    \"\"\"\n",
    "    Categorize ports into Well-Known Ports, Registered Ports, and Dynamic/Private Ports.\n",
    "\n",
    "    - Well-Known Ports: 0–1023\n",
    "    - Registered Ports: 1024–49151\n",
    "    - Dynamic/Private Ports: 49152–65535\n",
    "    \"\"\"\n",
    "    if port < 1024:\n",
    "        return 'Well-Known Ports'\n",
    "    elif 1024 <= port <= 49151:\n",
    "        return 'Registered Ports'\n",
    "    else:\n",
    "        return 'Dynamic/Private Ports'\n",
    "\n",
    "    # Define protocol categories"
   ],
   "id": "381ed6dd9dfd73d3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:29.331902Z",
     "start_time": "2025-02-09T00:19:29.327764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_ports_protocols_col_toDF(df):\n",
    "    # Correctly applying the categorization function to each port in 'Src Port'\n",
    "    df['SrcPort_categorical'] = df['Src Port'].apply(\n",
    "        lambda port: categorize_ports(port) if port is not None else None)\n",
    "\n",
    "    # Correctly applying the protocol naming function to each protocol in 'Protocol'\n",
    "    df['Protocol_categorical'] = df['Protocol'].apply(\n",
    "        lambda protocol: get_protocol_name(protocol) if protocol is not None else None)"
   ],
   "id": "8c1ed1d09ca302d9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:30.923208Z",
     "start_time": "2025-02-09T00:19:29.333474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def csv_to_df(csv_file, dir=CSV_DIR):\n",
    "    df = pd.read_csv(f\"{dir}/{csv_file}\")\n",
    "    df._filename = csv_file\n",
    "    return df\n",
    "\n",
    "\n",
    "## Monday\n",
    "df_pvt_monday = csv_to_df(\"enp0s3-monday-pvt.pcap_Flow.csv\")\n",
    "df_monday = csv_to_df(\"enp0s3-monday.pcap_Flow.csv\")\n",
    "\n",
    "## Tuesday\n",
    "df_pvt_tuesday = csv_to_df(\"enp0s3-pvt-tuesday.pcap_Flow.csv\")\n",
    "df_public_tuesday = csv_to_df(\"enp0s3-public-tuesday.pcap_Flow.csv\")\n",
    "\n",
    "## Wednesday\n",
    "df_pvt_wednesday = csv_to_df(\"enp0s3-pvt-wednesday.pcap_Flow.csv\")\n",
    "df_public_wednesday = csv_to_df(\"enp0s3-public-wednesday.pcap_Flow.csv\")\n",
    "\n",
    "## Thursday\n",
    "df_pvt_thursday = csv_to_df(\"enp0s3-pvt-thursday.pcap_Flow.csv\")\n",
    "df_public_thursday = csv_to_df(\"enp0s3-public-thursday.pcap_Flow.csv\")\n",
    "\n",
    "## Friday\n",
    "df_tcpdump_pvt_friday = csv_to_df(\"enp0s3-tcpdump-pvt-friday.pcap_Flow.csv\")\n",
    "df_tcpdump_friday = csv_to_df(\"enp0s3-tcpdump-friday.pcap_Flow.csv\")\n"
   ],
   "id": "de4ae44eb3a9fe38",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:30.960547Z",
     "start_time": "2025-02-09T00:19:30.951953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def describe_df(df):\n",
    "    \"\"\"\n",
    "    Provides a comprehensive overview of a pandas DataFrame including its dimensions,\n",
    "    size, shape, preview of the first few rows, descriptive statistics for numeric\n",
    "    and categorical data, and information about missing values. Checks and reports if\n",
    "    the entire DataFrame or any column is entirely composed of missing values.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The DataFrame to describe.\n",
    "\n",
    "    Returns:\n",
    "    None: Prints the description to the console.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the entire DataFrame is null\n",
    "        if df.isnull().all().all():\n",
    "            print(\"The entire DataFrame is null.\")\n",
    "            return\n",
    "\n",
    "        # Basic DataFrame information\n",
    "        print(f\"DataFrame Description:\")\n",
    "        print(f\"Dimensions: {df.ndim}\")\n",
    "        print(f\"Total elements: {df.size}\")\n",
    "        print(f\"Shape (rows, columns): {df.shape}\")\n",
    "        '''  print(\"\\nFirst 3 rows of the DataFrame:\")\n",
    "        print(df.head(3))'''\n",
    "\n",
    "        # Descriptive statistics\n",
    "        print(\"\\nDescriptive Statistics for Numeric Columns:\")\n",
    "        print(df.describe(include=\"number\"))\n",
    "        print(\"\\nDescriptive Statistics for Categorical Columns:\")\n",
    "        print(df.describe(include=\"object\"))\n",
    "        \n",
    "         # Display unique source details\n",
    "        UniqueSrcIP = pd.DataFrame({'Src IP': df['SrcIP_uniq']})\n",
    "        print(\"Unique Src Details:\")\n",
    "        description_src = UniqueSrcIP['Src IP'].describe(include='all')\n",
    "        print(f\"Uniq Src-IP's Describe:\\n{description_src}\")\n",
    "        print(f\"Uniq Src-IP's Top Value:\\n{description_src['top']} (Count: {description_src['freq']})\\n\")\n",
    "        print(f\"Uniq Src-IP's Full List:\\n{UniqueSrcIP.head()}\\n\")\n",
    "\n",
    "        # Ports\n",
    "        UniqueSrcPorts = pd.DataFrame({'Src Port': df['SrcPort_uniq']})\n",
    "        description_ports = UniqueSrcPorts['Src Port'].describe(include='all')\n",
    "\n",
    "        # Ensure that 'top' and 'freq' are present in the description\n",
    "        top_value = description_ports.get('top', 'No data')\n",
    "        freq_value = description_ports.get('freq', 'No data')\n",
    "\n",
    "        print(f\"Uniq Src-Ports Top Value:\\n{top_value} (Count: {freq_value})\\n\")\n",
    "        print(f\"Uniq Src-Ports Full List:\\n{UniqueSrcPorts.head()}\\n\")\n",
    "\n",
    "        # Protocol\n",
    "        UniqueProtocol = pd.DataFrame({'Protocol': df['Protocol_uniq']})\n",
    "        description_portss = UniqueProtocol['Protocol'].describe(include='all')\n",
    "\n",
    "        # Ensure that 'top' and 'freq' are present in the description\n",
    "        top_value = description_portss.get('top', 'No data')\n",
    "        freq_value = description_portss.get('freq', 'No data')\n",
    "\n",
    "        print(f\"Uniq Protocol Top Value:\\n{top_value} (Count: {freq_value})\\n\")\n",
    "        print(f\"Uniq Protocol Full List:\\n{UniqueProtocol.head()}\\n\")\n",
    "\n",
    "        '''for col in df.columns:\n",
    "            # Check if the column contains lists\n",
    "            if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "                # Flatten the list column into a single list for value counts\n",
    "                flattened_values = [item for sublist in df[col] for item in sublist]\n",
    "                flattened_series = pd.Series(flattened_values)\n",
    "\n",
    "                # Perform value counts and describe\n",
    "                value_counts = flattened_series.value_counts()\n",
    "                print(f\"Column: {col}\")\n",
    "                print(\"Value Counts Description:\")\n",
    "                print(value_counts.describe())\n",
    "            '''    \n",
    "\n",
    "        ''' # Information on missing data\n",
    "        print(\"\\nMissing Data Status (first few rows):\")\n",
    "        print(df.isna().head())\n",
    "        print(\"\\nCount of missing values in each column:\")\n",
    "        missing_data = df.isna().sum()'''\n",
    "\n",
    "        '''# Check for columns that are entirely null\n",
    "        all_null_columns = missing_data[missing_data == len(df)].index.tolist()\n",
    "        if all_null_columns:\n",
    "            print(f\"Columns entirely composed of missing values: {all_null_columns}\")\n",
    "        else:\n",
    "            print(\"No columns are entirely null.\")'''\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error processing DataFrame:\", str(e))\n",
    "    else:\n",
    "        print(\"____________________________________________________________________________\\n\")"
   ],
   "id": "ade9cf6ee95012d9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:30.966935Z",
     "start_time": "2025-02-09T00:19:30.962482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_zscore_outliers_iqr(df, column):\n",
    "    \"\"\"Detects outliers for a threshold in a DataFrame column using the IQR method.\"\"\"\n",
    "    # Using ZScore to normalize the data - considering more robust or tailored approaches to detect outliers.\n",
    "    zscore = stats.zscore(df[column])\n",
    "    df[f'Normalized_{column}'] = zscore\n",
    "    iqr = 0.5\n",
    "    lower_bound = 0.25 - 1.5 * iqr\n",
    "    upper_bound = 0.75 + 1.5 * iqr\n",
    "\n",
    "    outlier_dict = {\n",
    "        \"dict_name\": \"outlier_Normalized_dict\",\n",
    "        \"column\": column,\n",
    "        \"lower_bound\": lower_bound,\n",
    "        \"upper_bound\": upper_bound,\n",
    "        \"iqr\": iqr,\n",
    "        \"std\": df[f\"Normalized_{column}\"].std()\n",
    "    }\n",
    "    return outlier_dict"
   ],
   "id": "607fc0f6fc313cfe",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:30.977487Z",
     "start_time": "2025-02-09T00:19:30.973158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_outliers_iqr(df, column):\n",
    "    # Normalized Outlier\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    iqr = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * iqr\n",
    "    upper_bound = Q3 + 1.5 * iqr\n",
    "\n",
    "    outlier_dict = {\n",
    "        \"dict_name\": \"outlier_dict\",\n",
    "        # \"df_pointer\": point_to_df,\n",
    "        \"column\": column,\n",
    "        \"lower_bound\": lower_bound,\n",
    "        \"upper_bound\": upper_bound,\n",
    "        \"iqr\": iqr,\n",
    "        \"std\": df[column].std()\n",
    "    }\n",
    "    return outlier_dict"
   ],
   "id": "189a35a0b0dd3ec",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:30.986405Z",
     "start_time": "2025-02-09T00:19:30.979681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_ifOutlier_columns(df, column, CSV_filename=None):\n",
    "    if f'Outlier_Normalized_{column}_U/L' not in df:\n",
    "        outlier_dict = detect_zscore_outliers_iqr(df, column)\n",
    "        df[f'Outlier_Normalized_{column}_U/L'] = np.where(df[f'Normalized_{column}'] < outlier_dict['lower_bound'],\n",
    "                                                          'lower_bound', np.where(\n",
    "                df[f'Normalized_{column}'] > outlier_dict['upper_bound'],\n",
    "                'upper_bound', None))\n",
    "\n",
    "    if f'Outlier_{column}_U/L' not in df:\n",
    "        outlier_dict = detect_outliers_iqr(df, column)\n",
    "        df[f'Outlier_{column}_U/L'] = (\n",
    "            np.where((outlier_dict['lower_bound'] >= 0) and df[column] < outlier_dict['lower_bound'],\n",
    "                     'lower_bound', np.where(df[column] > outlier_dict['upper_bound'], 'upper_bound', None)))\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row.get(f'Outlier_Normalized_{column}_U/L') or row.get(f'Outlier_{column}_U/L'):\n",
    "            print(f\"There are outlier values for column {column} of grouped DF of {CSV_filename}.\")\n",
    "        return"
   ],
   "id": "1b58565177ee5522",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.000935Z",
     "start_time": "2025-02-09T00:19:30.988561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def group_data(df):\n",
    "    add_ports_protocols_col_toDF(df)\n",
    "\n",
    "    # Parse the full datetime\n",
    "    df['Datetime'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %I:%M:%S %p')\n",
    "\n",
    "    # Set 'Datetime' as the index\n",
    "     # Set 'Datetime' as the index\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "\n",
    "    # Group data:\n",
    "    # Merge the summarized data and detailed data using the 'group_id' key.\n",
    "    # The 'group_id' serves as the unique identifier connecting the summarized statistics\n",
    "    # with the detailed information about the sources (Src IP, Src Port, Protocol):\n",
    "    df['group_DosDetect_id'] = df.groupby(['Dst IP', pd.Grouper(freq='5min')]).ngroup()\n",
    "\n",
    "    # - 'grouped_summary': Contains aggregate statistics (e.g., counts, averages) per Dst IP and time interval.\n",
    "    grouped_summary = df.groupby(['group_DosDetect_id', 'Dst IP', pd.Grouper(freq='5min')]).agg(\n",
    "        # Aggregate summarized statistics per group\n",
    "        CountRequests=('Src IP', 'count'),\n",
    "        CountSrc_uniq=('Src IP', 'nunique'),  # Count unique source IPs\n",
    "        Flow_Packets_s_avg=('Flow Packets/s', 'mean'),\n",
    "        Flow_Bytes_s_avg=('Flow Bytes/s', 'mean'),\n",
    "        SYN_count_sum=('SYN Flag Count', 'sum'),\n",
    "        ACK_count_sum=('ACK Flag Count', 'sum'),\n",
    "        Bwd_sum=('Total Bwd packets', 'sum'),\n",
    "        Fwd_sum=('Total Fwd Packet', 'sum'),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculating ratios separately to avoid referencing within the aggregation function\n",
    "    grouped_summary['SYN_ACK_Ratio'] = grouped_summary['SYN_count_sum'] /((grouped_summary['ACK_count_sum']) + 1)\n",
    "    grouped_summary['ACK_SYN_Ratio'] = grouped_summary['ACK_count_sum'] / (grouped_summary['SYN_count_sum'] + 1)\n",
    "    grouped_summary['Bwd_Fwd_Ratio'] = grouped_summary['Bwd_sum'] / (grouped_summary['Fwd_sum'] + 1)\n",
    "\n",
    "    # Format 'HourTime' as a datetime column for sorting\n",
    "    grouped_summary['HourTime'] = grouped_summary['Datetime'].dt.floor('30min').dt.strftime('%H:%M')\n",
    "    # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "    grouped_summary = grouped_summary.sort_values(by='Dst IP')\n",
    "    grouped_summary = grouped_summary.sort_values(by='HourTime')\n",
    "\n",
    "    # Detect outliers using IQR\n",
    "    for col in ['CountRequests', 'Flow_Packets_s_avg', 'Flow_Bytes_s_avg', 'SYN_count_sum', 'ACK_count_sum',\n",
    "                'SYN_ACK_Ratio', 'ACK_SYN_Ratio', 'Bwd_sum', 'Fwd_sum', 'Bwd_Fwd_Ratio']:\n",
    "        add_ifOutlier_columns(grouped_summary, col, df._filename)\n",
    "\n",
    "    # - 'grouped_src_details':\n",
    "    # Contains detailed lists of Src IPs, their ports, and protocols associated with each group.\n",
    "    # Extracting unique source details per group\n",
    "    grouped_src_details = df.groupby('group_DosDetect_id').apply(\n",
    "        lambda group: pd.Series({\n",
    "            'Src Details': group[\n",
    "                ['Src IP', 'Src Port', 'SrcPort_categorical', 'Protocol', 'Protocol_categorical']].to_dict('records'),\n",
    "            'SrcIP_uniq': group['Src IP'].unique().tolist(),\n",
    "            'SrcPort_uniq': group['Src Port'].unique().tolist(),\n",
    "            'SrcPort_categorical': group['SrcPort_categorical'].unique().tolist(),\n",
    "            'Protocol_uniq': group['Protocol'].unique().tolist(),\n",
    "            'Protocol_categorical': group['Protocol_categorical'].unique().tolist(),\n",
    "            \n",
    "            # count of port types\n",
    "            'Well_Known_Port_Count': sum(group['SrcPort_categorical'] == 'Well-Known Ports'),\n",
    "            'Registered_Port_Count': sum(group['SrcPort_categorical'] == 'Registered Ports'),\n",
    "            'Dynamic_Private_Port_Count': sum(group['SrcPort_categorical'] == 'Dynamic/Private Ports'),\n",
    "\n",
    "            # count of protocol types\n",
    "            'Protocol_TCP_Count': sum(group['Protocol_categorical'] == 'TCP'),\n",
    "            'Protocol_UDP_Count': sum(group['Protocol_categorical'] == 'UDP'),\n",
    "            'Protocol_ICMP_Count': sum(group['Protocol_categorical'] == 'ICMP'),\n",
    "            'Other_Protocol_Count': sum(group['Protocol_categorical'] == 'Other'),\n",
    "            })\n",
    "    ).reset_index()\n",
    "\n",
    "    # - 'Src Details': Contains detailed dictionaries with Src IP, Src Port, and Protocol for each group to check their sets.\n",
    "    # - 'SrcPort_uniq': Lists all unique source ports within the group.\n",
    "    # - 'SrcPort_categorical': Groups source ports into predefined categories (Well-Known, Registered, Dynamic/Private).\n",
    "    # - 'Protocol_uniq': Lists all unique protocol numbers within the group.\n",
    "    # - 'Protocol_categorical': Maps protocol numbers to human-readable protocol names.\n",
    "    # - 'Well_Known_Port_Count'/'Registered_Port_Count'/'Dynamic_Private_Port_Count' :count of port types.\n",
    "    # - 'Protocol_TCP_Count'/'Protocol_UDP_Count'/'Protocol_ICMP_Count'/'Other_Protocol_Count': count of protocol types.\n",
    "            \n",
    "\n",
    "    # By performing the merge:\n",
    "    # A combined DataFrame where each row provides both high-level statistics and source-level details for analysis.\n",
    "    merged_data = grouped_summary.merge(grouped_src_details, on='group_DosDetect_id', how='inner')\n",
    "    # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "    merged_data = merged_data.sort_values(by='Dst IP')\n",
    "    merged_data = merged_data.sort_values(by='HourTime')\n",
    "\n",
    "    return merged_data"
   ],
   "id": "da3f00a73a277553",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.018628Z",
     "start_time": "2025-02-09T00:19:31.003204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def graph_for_outlierCol(df, outlier_dict, normData=None, outlierNormal_dict=None, attack=None):\n",
    "    column = outlier_dict['column']\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column in df.columns:\n",
    "        # Ensure 'HourTime' is properly formatted as datetime for sorting and plotting\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['HourTime']):\n",
    "            df['HourTime'] = pd.to_datetime(df['Datetime']).dt.floor('30min').dt.strftime('%H:%M')\n",
    "        # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "        df = df.sort_values(by='Dst IP')\n",
    "        df = df.sort_values(by='HourTime')\n",
    "\n",
    "        # Calculate the necessary statistics for the column\n",
    "        col_mid = df[column].median()\n",
    "        # col_std = df[column].std()\n",
    "        lower_bound = outlier_dict['lower_bound']\n",
    "        upper_bound = outlier_dict['upper_bound']\n",
    "\n",
    "        # Plot the data with horizontal lines for thresholds\n",
    "        plt.figure(figsize=(18, 10))\n",
    "        sns.lineplot(data=df, x='HourTime', y=column, marker='o', color='y', label='Analysed Data')\n",
    "        # Add horizontal lines to indicate statistical thresholds\n",
    "        plt.axhline(y=col_mid, color='blue', linestyle='dashed', linewidth=2, label='Median')\n",
    "        # plt.axhline(y=col_mid - col_std, color='sky blue', linestyle='dashed', linewidth=2, label='-1 STD')\n",
    "        # plt.axhline(y=col_mid + col_std, color='sky blue', linestyle='dashed', linewidth=2, label='+1 STD')\n",
    "        plt.axhline(y=lower_bound, color='y', linestyle='dashed', linewidth=2, label='Lower Bound')\n",
    "        plt.axhline(y=upper_bound, color='y', linestyle='dashed', linewidth=2, label='Upper Bound')\n",
    "        # Add titles, labels, and legend for clarity\n",
    "        plt.title(f\"{column} Over Time with Horizontal Thresholds\", fontsize=14,\n",
    "                  fontweight='bold')\n",
    "        plt.xlabel(\"Time (Hour:Minute)\", fontsize=12)\n",
    "        plt.ylabel(f\"{column} Values\", fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        flag = False\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        # Normal data\n",
    "        if not normData.empty:\n",
    "            # Ensure the column exists in the DataFrame\n",
    "            if column in normData.columns:\n",
    "                flag = True\n",
    "                # Ensure 'HourTime' is properly formatted as datetime for sorting and plotting\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df['HourTime']):\n",
    "                    normData['HourTime'] = pd.to_datetime(normData['Datetime']).dt.floor('30min').dt.strftime('%H:%M')\n",
    "                # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "                normData = normData.sort_values(by='Dst IP')\n",
    "                normData = normData.sort_values(by='HourTime')\n",
    "                sns.lineplot(data=df, x='HourTime', y=column, marker='o', color='y', label='Compared Data')\n",
    "                # Calculate the necessary statistics for the column from normal data\n",
    "                n_col_mid = normData[column].median()\n",
    "                # n_col_std = normData[nColumn].std()\n",
    "                lower_nBound = outlierNormal_dict['lower_bound']\n",
    "                upper_nBound = outlierNormal_dict['upper_bound']\n",
    "\n",
    "                sns.lineplot(data=normData, x='HourTime', y=column, marker='o', color='c', label='Normal Data')\n",
    "                # Add horizontal lines to indicate statistical thresholds\n",
    "                plt.axhline(y=n_col_mid, color='r', linestyle='dashed', linewidth=2, label='Normal Median')\n",
    "                # plt.axhline(y=n_col_mid - n_col_std, color='gray', linestyle='dashed', linewidth=2, label='-1 Normal STD')\n",
    "                # plt.axhline(y=n_col_mid + n_col_std, color='gray', linestyle='dashed', linewidth=2, label='+1 Normal STD')\n",
    "                plt.axhline(y=lower_nBound, color='g', linestyle='dashed', linewidth=2, label='Lower Normal Bound')\n",
    "                plt.axhline(y=upper_nBound, color='g', linestyle='dashed', linewidth=2, label='Upper Normal Bound')\n",
    "                # Add titles, labels, and legend for clarity\n",
    "                plt.title(f\"{column} Over Time with Horizontal Thresholds:\\nNormal Data vs Compared Data\", fontsize=14,\n",
    "                          fontweight='bold')\n",
    "        else:\n",
    "            sns.lineplot(data=df, x='HourTime', y=column, marker='o', color=\"orange\", label='Analysed Data')\n",
    "            # Add horizontal lines to indicate statistical thresholds\n",
    "            plt.axhline(y=col_mid, color='blue', linestyle='dashed', linewidth=2, label='Median')\n",
    "            # plt.axhline(y=col_mid - col_std, color='sky blue', linestyle='dashed', linewidth=2, label='-1 STD')\n",
    "            # plt.axhline(y=col_mid + col_std, color='sky blue', linestyle='dashed', linewidth=2, label='+1 STD')\n",
    "            plt.axhline(y=lower_bound, color='y', linestyle='dashed', linewidth=2, label='Lower Bound')\n",
    "            plt.axhline(y=upper_bound, color='y', linestyle='dashed', linewidth=2, label='Upper Bound')\n",
    "            # Add titles, labels, and legend for clarity\n",
    "            plt.title(f\"{column} Over Time with Horizontal Thresholds\", fontsize=14,\n",
    "                      fontweight='bold')\n",
    "        if attack is not None and not attack.empty:\n",
    "            #and (attack.group_DosDetect_id in df.group_DosDetect_id):\n",
    "            if column in attack.columns:\n",
    "                flag = True\n",
    "                # Ensure 'HourTime' is properly formatted as datetime for sorting and plotting\n",
    "                if not np.issubdtype(attack['HourTime'].dtype, np.datetime64):\n",
    "                    attack['HourTime'] = attack['Datetime'].dt.floor('30min').dt.strftime('%H:%M')\n",
    "                # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "                attack = attack.sort_values(by='Dst IP')\n",
    "                attack = attack.sort_values(by='HourTime')\n",
    "                sns.lineplot(data=attack, x='HourTime', y=column, marker='o', color=\"orange\", label=f'attack_{attack['attack_type']}')\n",
    "        if flag:#if we have an attack to show or a normal data\n",
    "            plt.xlabel(\"Time (Hour:Minute)\", fontsize=12)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel(f\"{column} Values\", fontsize=12)\n",
    "            plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # --- Notes for Horizontal Lines ---\n",
    "        # Horizontal lines highlight statistical thresholds (median, ±std, and bounds) on the y-axis.\n",
    "        # This is useful to identify patterns or anomalies in the column's value distribution over time.\n",
    "\n",
    "    else:\n",
    "        return None"
   ],
   "id": "f0c3bf0235306af",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.027614Z",
     "start_time": "2025-02-09T00:19:31.020613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def graphs_for_outliers(df, normal_data=None, attack=None):\n",
    "    flag = True\n",
    "    zscore_N_dict = None\n",
    "    outlier_N_dict = None\n",
    "    for col in df.columns:\n",
    "        for _, row in df.iterrows():\n",
    "            if (row.get(f'Outlier_Normalized_{col}_U/L') or row.get(f'Outlier_{col}_U/L')) and flag:\n",
    "                # Transform the dictionaries into DataFrames\n",
    "                zscore_dict = detect_zscore_outliers_iqr(df, col)\n",
    "                outlier_dict = detect_outliers_iqr(df, col)\n",
    "                df1 = pd.DataFrame.from_dict(zscore_dict, orient='index', columns=['zscore_values'])\n",
    "                df2 = pd.DataFrame.from_dict(outlier_dict, orient='index', columns=['outlier_values'])\n",
    "                # Combine the DataFrames (e.g., vertically or horizontally)\n",
    "                # Horizontally: Adding columns\n",
    "                combined_df_horizontal = pd.concat([df1, df2], axis=1)\n",
    "                if normal_data.empty:\n",
    "                    print(combined_df_horizontal)\n",
    "                    flag = False\n",
    "                else:\n",
    "                    if row[col] > detect_zscore_outliers_iqr(normal_data, col)['upper_bound'] or row[\n",
    "                        f\"Normalized_{col}\"] > \\\n",
    "                            detect_outliers_iqr(normal_data, col)['upper_bound'] or row[col] < \\\n",
    "                            detect_zscore_outliers_iqr(normal_data, col)['lower_bound'] or row[f\"Normalized_{col}\"] < \\\n",
    "                            detect_outliers_iqr(normal_data, col)['lower_bound']:\n",
    "                        print(combined_df_horizontal)\n",
    "                        zscore_N_dict = detect_zscore_outliers_iqr(df, col)\n",
    "                        outlier_N_dict = detect_outliers_iqr(df, col)\n",
    "                        flag = False\n",
    "\n",
    "                graph_for_outlierCol(df, zscore_dict, normal_data, zscore_N_dict, attack)\n",
    "                graph_for_outlierCol(df, outlier_dict, normal_data, outlier_N_dict, attack)\n",
    "\n",
    "        if not flag:\n",
    "            print(f\"No Outlier values for {col} in this DF\")\n",
    "        flag = True"
   ],
   "id": "241d8f8af8d08d83",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.044135Z",
     "start_time": "2025-02-09T00:19:31.029916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def classify_attack(row, df_filename, dict_dos_attacks, key, normal_data=None):\n",
    "    # This function returns the type of attack based on the row data\n",
    "    attack_type = 'Normal'\n",
    "    column = None\n",
    "    Ratio = None\n",
    "\n",
    "    # Classifies the type of DoS to ensure the anomaly represents an attack:\n",
    "    # Calls detect_outliers_iqr_dict() for classification.\n",
    "\n",
    "    if row['Outlier_Normalized_SYN_count_sum_U/L'] == \"upper_bound\" and \\\n",
    "            row['Outlier_SYN_ACK_Ratio_U/L'] == \"upper_bound\" and \\\n",
    "            row['Outlier_Flow_Packets_s_avg_U/L'] == \"upper_bound\":\n",
    "        flag = True\n",
    "        \"\"\"\n",
    "            SYN Flood Attack\n",
    "            Explanation of SYN Flood:\n",
    "            TCP Protocol \"3 Way Handshake\" exploitation attack.\n",
    "\n",
    "            An attacker sends a large number of SYN packets to initiate a TCP connection but doesn't complete \n",
    "            the \"3 Way Handshake\" by sending ACK packets. This behavior causes server resources to be exhausted\n",
    "            because connections remain incomplete.\n",
    "\n",
    "            Flow_Packets/s >> upper bound:\n",
    "\n",
    "            The target is overwhelmed by the high volume of packets rather than their payload size.\n",
    "\n",
    "            High SYN/ACK Ratio indicates SYN Flood:\n",
    "        \"\"\"\n",
    "        if not normal_data.empty:\n",
    "            columns_to_check = ['Normalized_SYN_count_sum', 'Normalized_SYN_ACK_Ratio', 'Flow_Packets_s_avg']\n",
    "            flag = False\n",
    "            for column in columns_to_check:\n",
    "                # Adjusted condition to check against normal thresholds\n",
    "                if \"Normalized\" in column:\n",
    "                    if row[column] > detect_zscore_outliers_iqr(normal_data, column)['upper_bound']:\n",
    "                        flag = True\n",
    "                        break  # Assuming any single outlier condition triggers the classification\n",
    "                else:\n",
    "                    if row[column] > detect_outliers_iqr(normal_data, column)['upper_bound']:\n",
    "                        flag = False\n",
    "                        break  # Assuming any single outlier condition triggers the classification\n",
    "        if flag:\n",
    "            attack_type = 'SYN_Flood'\n",
    "            column = 'SYN_ACK_Ratio'\n",
    "            Ratio = row['SYN_ACK_Ratio']\n",
    "\n",
    "        elif row[f\"Outlier_Normalized_CountRequests_U/L\"] == \"upper_bound\" and \\\n",
    "                row[\"Outlier_Normalized_Bwd_Fwd_Ratio_U/L\"] == \"upper_bound\" and \\\n",
    "                row[\"Outlier_Normalized_Bwd_sum_U/L\"] == \"upper_bound\" and \\\n",
    "                row['Outlier_Flow_Packets_s_avg_U/L'] == \"upper_bound\" and \\\n",
    "                row['Outlier_Flow_Bytes_s_avg_U/L'] == \"upper_bound\":\n",
    "            flag = True\n",
    "            \"\"\"  \n",
    "                Reflection Attack (Amplification)\n",
    "                just outlier and mot normalize because we want to understand if its big not huge - fix explenation\n",
    "                Explanation of Reflection Attack:\n",
    "                Usually happens in UDP Protocol exploitation attack. - fix explenation\n",
    "\n",
    "                The attacker uses third-party servers (e.g., DNS, NTP) to reflect requests towards the victim. \n",
    "                By spoofing the victim's IP address, the attacker tricks servers into sending large responses \n",
    "                to the victim, overwhelming its resources.\n",
    "                This method often leverages stateless ports and services, such as DNS, NTP, or SSDP.\n",
    "\n",
    "                Backward traffic (Bwd) is significantly higher due to the large volume of responses sent to the victim.\n",
    "                Check for outliers and normal thresholds\n",
    "            \"\"\"\n",
    "            if not normal_data.empty:\n",
    "                columns_to_check = ['Normalized_CountRequests', 'Normalized_Bwd_Fwd_Ratio', 'Bwd_sum',\n",
    "                                    'Flow_Packets_s_avg', 'Flow_Bytes_s_avg']\n",
    "                flag = False\n",
    "                for column in columns_to_check:\n",
    "                    # Adjusted condition to check against normal thresholds\n",
    "                    if \"Normalized\" in column:\n",
    "                        if row[column] > detect_zscore_outliers_iqr(normal_data, column)['upper_bound']:\n",
    "                            flag = True\n",
    "                            break  # Assuming any single outlier condition triggers the classification\n",
    "                    else:\n",
    "                        if row[column] > detect_outliers_iqr(normal_data, column)['upper_bound']:\n",
    "                            flag = False\n",
    "                            break  # Assuming any single outlier condition triggers the classification\n",
    "            if flag:\n",
    "                attack_type = 'Reflection_Attack'\n",
    "                column = 'Bwd_Fwd_Ratio'\n",
    "                Ratio = row['Bwd_Fwd_Ratio']\n",
    "\n",
    "        # ACK Flood Attack\n",
    "        elif row[\"Outlier_Normalized_ACK_count_sum_U/L\"] == \"upper_bound\" and \\\n",
    "                row['Outlier_ACK_SYN_Ratio_U/L'] == \"upper_bound\" and \\\n",
    "                row['Outlier_Flow_Packets_s_avg_U/L'] == \"upper_bound\" and \\\n",
    "                row['Outlier_SYN_count_sum_U/L'] == \"lower_bound\":\n",
    "            flag = True\n",
    "            \"\"\" \n",
    "                ACK Flood Attack\n",
    "                TCP Protocol exploitation attack.\n",
    "\n",
    "                The attacker sends a high volume of ACK packets, often with little or no payload,\n",
    "                to overwhelm the target. These packets require the target to process each ACK,\n",
    "                causing a load on its resources.\n",
    "\n",
    "                # Flow_Packets/s >> upper bound:\n",
    "                The attack is effective because of the sheer number of packets rather than their size or payload.\n",
    "                # Flow_Bytes/s >> lower bound:\n",
    "                The payload of these packets is minimal or even empty, making the focus entirely on the number of packets.\n",
    "                # Fwd/Bwd Ratio ≈ 1:\n",
    "                The Fwd/Bwd ratio is close to 1 because each packet sent to the target\n",
    "                results in a nearly identical response, creating balanced traffic.\n",
    "            \"\"\"\n",
    "            if not normal_data.empty:\n",
    "                columns_to_check = ['Normalized_ACK_count_sum', 'Normalized_ACK_SYN_Ratio', 'Flow_Packets_s_avg']\n",
    "                flag = False\n",
    "                for column in columns_to_check:\n",
    "                    # Adjusted condition to check against normal thresholds\n",
    "                    if \"Normalized\" in column:\n",
    "                        if row[column] > detect_zscore_outliers_iqr(normal_data, column)['upper_bound']:\n",
    "                            flag = True\n",
    "                            break  # Assuming any single outlier condition triggers the classification\n",
    "                    else:\n",
    "                        if row[column] > detect_outliers_iqr(normal_data, column)['upper_bound']:\n",
    "                            flag = False\n",
    "                            break  # Assuming any single outlier condition triggers the classification\n",
    "            if flag:\n",
    "                attack_type = 'Reflection_Attack'\n",
    "                column = 'Bwd_Fwd_Ratio'\n",
    "                Ratio = row['Bwd_Fwd_Ratio']\n",
    "\n",
    "    if attack_type != 'Normal':\n",
    "        dict_dos_attacks[key] = {\n",
    "            \"Attack_id\": f\"{key}-{row.group_DosDetect_id}\",\n",
    "            \"group_DosDetect_id\": row.group_DosDetect_id,\n",
    "            \"Source_dfName\": df_filename,\n",
    "            \"attack_type\": attack_type,\n",
    "            f\"{column}\": Ratio,\n",
    "            \"Dst IP\": row['Dst IP'],\n",
    "            \"CountRequests\": row['CountRequests'],\n",
    "            \"CountSrc_uniq\": row['CountSrc_uniq'],  # Count unique source IPs\n",
    "            \"Datetime\": row['Datetime'],\n",
    "            \"HourTime\": row['HourTime'],\n",
    "            # Calculate statistics for SYN Flag Count\n",
    "            \"SYN_count_sum\": row.SYN_count_sum,\n",
    "            # Calculate statistics for ACK Flag Count\n",
    "            \"ACK_count_sum\": row.ACK_count_sum,\n",
    "            # Calculate statistics for Fwd Packet Flag Count\n",
    "            \"Fwd_sum\": row.Fwd_sum,\n",
    "            # Calculate statistics for Bwd Packet Flag Count\n",
    "            \"Bwd_sum\": row.Bwd_sum,\n",
    "            # Avg Flow Packets/ Bytes\n",
    "            \"Flow_Packets_s_avg\": row.Flow_Packets_s_avg,\n",
    "            \"Flow_Bytes_s_avg\": row.Flow_Bytes_s_avg,\n",
    "            # Source details\n",
    "            \"Src Details\": row['Src Details'],\n",
    "            \"SrcIP_uniq\": row['SrcIP_uniq'],\n",
    "            \"SrcPort_uniq\": row.SrcPort_uniq,\n",
    "            \"SrcPort_categorical\": row.SrcPort_categorical,\n",
    "            \"Protocol_uniq\": row.Protocol_uniq,\n",
    "            \"Protocol_categorical\": row.Protocol_categorical,\n",
    "            # Port categorial count\n",
    "            'Well_Known_Port_Count': row.Well_Known_Port_Count,\n",
    "            'Registered_Port_Count': row.Registered_Port_Count,\n",
    "            'Dynamic_Private_Port_Count': row.Dynamic_Private_Port_Count,\n",
    "            # Protocol categorial count\n",
    "            'Protocol_TCP_Count': row.Protocol_TCP_Count,\n",
    "            'Protocol_UDP_Count': row.Protocol_UDP_Count,\n",
    "            'Protocol_ICMP_Count': row.Protocol_TCP_Count,\n",
    "            'Other_Protocol_Count': row.Other_Protocol_Count,\n",
    "        }"
   ],
   "id": "b2c3119cbb98ad70",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.059693Z",
     "start_time": "2025-02-09T00:19:31.047016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Reflection_Attack(df_of_attack, df):\n",
    "    # Ensuring the filter data for Reflection Attacks for graphs \n",
    "    reflection_attacks = df_of_attack[df_of_attack['attack_type'] == 'Reflection_Attack']\n",
    "    if not np.issubdtype(reflection_attacks['HourTime'].dtype, np.datetime64):\n",
    "                    reflection_attacks['HourTime'] = reflection_attacks['Datetime'].dt.floor('30min').dt.strftime('%H:%M')\n",
    "    # Sort Reflection Attack data by 'HourTime'\n",
    "    reflection_attacks = reflection_attacks.sort_values(by='Dst IP')\n",
    "    reflection_attacks = reflection_attacks.sort_values(by='HourTime')\n",
    "    \n",
    "    # Plot Flow_Packets_s_avg over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=reflection_attacks, x='HourTime', y='Flow_Packets_s_avg', marker='o')\n",
    "    plt.title(\"Flow Packets Per Hour During Reflection Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Packets/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Flow_Bytes_s_avg over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=reflection_attacks, x='HourTime', y='Flow_Bytes_s_avg', marker='o')\n",
    "    plt.title(\"Flow Bytes Per Hour During Reflection Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Byte/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Flow Packets and Bytes Over Time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=reflection_attacks, x='HourTime', y='Flow_Packets_s_avg', marker='o', label='Flow Packets')\n",
    "    sns.lineplot(data=reflection_attacks, x='HourTime', y='Flow_Bytes_s_avg', marker='o', label='Flow Bytes')\n",
    "    plt.title(\"Flow Packets and Bytes Over Time During Reflection Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Packets/s and Byte/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Bwd and Fwd Flow Packets Over Time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=reflection_attacks, x='HourTime', y='Bwd_sum', marker='o', label='Bwd Packet')\n",
    "    sns.lineplot(data=reflection_attacks, x='HourTime', y='Fwd_sum', marker='o', label='Fwd Packet')\n",
    "    plt.title(\"Bwd and Fwd Flow Packets Over Time During Reflection Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Packets/s and Byte/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Flow Packets and Bytes Over Time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=reflection_attacks, x='HourTime', y='Flow_Packets_s_avg', marker='o',\n",
    "                 label='Flow Packets - Reflection Attack')\n",
    "    sns.lineplot(data=reflection_attacks, x='HourTime', y='Flow_Bytes_s_avg', marker='o',\n",
    "                 label='Flow Bytes - Reflection Attack')\n",
    "    sns.lineplot(data=df, x='HourTime', y='Flow_Bytes_s_avg', marker='o', label='Flow Packets - All comunication')\n",
    "    sns.lineplot(data=df, x='HourTime', y='Flow_Bytes_s_avg', marker='o', label='Flow Bytes - All comunication')\n",
    "    plt.title(\"Flow Packets and Bytes Over Time During Reflection Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Packets/s and Byte/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Bwd and Fwd Flow Packets Over Time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # sns.lineplot(data=reflection_attacks, x='HourTime', y='Bwd_sum', marker='o', label='Bwd Packet - Reflection Attack')\n",
    "    # sns.lineplot(data=reflection_attacks, x='HourTime', y='Fwd_sum', marker='o', label='Fwd Packet - Reflection Attack')\n",
    "    sns.lineplot(data=df, x='HourTime', y='Bwd_sum', marker='o', label='Bwd Packets - All comunication')\n",
    "    sns.lineplot(data=df, x='HourTime', y='Fwd_sum', marker='o', label='Fwd Packet - All comunication')\n",
    "    plt.title(\"Bwd and Fwd Flow Packets Over Time During Reflection Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Packets/s and Byte/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    '''plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df, x='Dst IP', y='CountSrc_uniq', palette='coolwarm')\n",
    "    plt.title(\"Unique Source IPs Per Destination\\nDuring Reflection Attacks\")\n",
    "    plt.xlabel(\"Destination IP\")\n",
    "    plt.ylabel(\"Unique Source Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()'''\n",
    "\n",
    "    '''# Violin plot for FwdPacket_count_sum\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.violinplot(data=reflection_attacks, y='FwdPacket_count_sum')\n",
    "    plt.title(\"Forward Packet Count Distribution During Reflection Attacks\")\n",
    "    plt.ylabel(\"Forward Packet Count\")\n",
    "    plt.show()\n",
    "\n",
    "    # Violin plot for BwdPacket_count_sum\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.violinplot(data=reflection_attacks, y='BwdPacket_count_sum')\n",
    "    plt.title(\"Backward Packet Count Distribution During Reflection Attacks\")\n",
    "    plt.ylabel(\"Backward Packet Count\")\n",
    "    plt.show()'''\n",
    "\n",
    "    '''plt.scatter(df.Bwd_sum, df.Fwd_sum, color=[\"purple\",\"green\"], alpha=0.3)\n",
    "    plt.scatter(reflection_attacks.Bwd_sum, reflection_attacks.Fwd_sum, color=\"purple\", alpha=0.3)\n",
    "    plt.title(\"Bwd and Fwd Packet count correlation\")\n",
    "    plt.xlabel(\"Bwd Packets\")\n",
    "    plt.ylabel(\"Fwd Packets\")\n",
    "    plt.show()'''\n",
    "\n",
    "    ''' plt.scatter(df.Flow_Packets_s_avg, df.Flow_Bytes_s_avg, color=\"purple\", alpha=0.3)\n",
    "    plt.title(\"Flow_Packets and Flow_Bytes (Payload) correlation\")\n",
    "    plt.xlabel(\"Flow Packets\")\n",
    "    plt.ylabel(\"Flow Bytes\")\n",
    "    plt.show()'''"
   ],
   "id": "a53e5ae3d3771dd7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.077481Z",
     "start_time": "2025-02-09T00:19:31.061466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def SYN_Flood(df_of_attack, df):\n",
    "    # Ensuring the filter data for SYN Flood Attack for graphs \n",
    "    SYN_Flood = df_of_attack[df_of_attack['attack_type'] == 'SYN_Flood']\n",
    "    # Ensure 'HourTime' is properly formatted as datetime for sorting and plotting\n",
    "    if not np.issubdtype(df['HourTime'].dtype, np.datetime64):\n",
    "        df['HourTime'] = df['Datetime'].dt.floor('30min').dt.strftime('%H:%M')\n",
    "    # Sort Reflection Attack data by 'HourTime'\n",
    "    SYN_Flood = SYN_Flood.sort_values(by='Dst IP')\n",
    "    SYN_Flood = SYN_Flood.sort_values(by='HourTime')\n",
    "    \n",
    "    # Plot Flow_Packets_s_avg over time\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.lineplot(data=df, x='HourTime', y='Flow_Packets_s_avg', marker='o')\n",
    "    sns.lineplot(data=SYN_Flood, x='HourTime', y='Flow_Packets_s_avg', marker='o', color='orange')\n",
    "    plt.title(\"Flow Packets Per Hour During SYN Flood Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Packet/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Flow_Bytes_s_avg over time\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.lineplot(data=df, x='HourTime', y='Flow_Bytes_s_avg', marker='o')\n",
    "    sns.lineplot(data=SYN_Flood, x='HourTime', y='Flow_Bytes_s_avg', marker='o', color='orange')\n",
    "    plt.title(\"Flow Bytes Per Hour During SYN Flood Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Byte/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Flow_Bytes vs Flow_Packets Over Time\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.lineplot(data=df, x='HourTime', y='Flow_Packets_s_avg', marker='o', label='Flow Packets - All comunication')\n",
    "    sns.lineplot(data=df, x='HourTime', y='Flow_Bytes_s_avg', marker='o', label='Flow Bytes - All comunication')\n",
    "    plt.title(\"Flow Packets vs Flow Bytes Over Time During SYN Flood Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Packets/s and Byte/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Bar chart showing CountRequests over time\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.barplot(data=df, x='HourTime', y='CountRequests', errorbar=None)\n",
    "    plt.title(\"Count of Requests Over Time During SYN Flood Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Count of Requests\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Bar chart showing SYN_count_sum over time\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.barplot(data=df, x='HourTime', y='SYN_count_sum', errorbar=None)\n",
    "    plt.title(\"SYN Count Over Time\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Count of SYN Flags During SYN Flood Attacks\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    '''# Violin plot for ACK_count_sum\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.violinplot(data=df, x='HourTime', y='ACK_count_sum')\n",
    "    sns.violinplot(data=SYN_Flood, x='HourTime', y='ACK_count_sum')\n",
    "    plt.title(\"ACK Flag Packet Count Distribution During SYN Flood Attack\")\n",
    "    plt.ylabel(\"ACK Flag Packet Count\")\n",
    "    plt.show()\n",
    "\n",
    "    # Violin plot for SYN_count_sum\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.violinplot(data=df, x='HourTime', y='SYN_count_sum')\n",
    "    sns.violinplot(data=SYN_Flood, x='HourTime', y='SYN_count_sum')\n",
    "    plt.title(\"SYN Packet Count Distribution During SYN Flood Attack\")\n",
    "    plt.ylabel(\"SYN Flag Packet Count\")\n",
    "    plt.show()\n",
    "    # Violin plot for SYN_count_sum\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.violinplot(data=df, x='HourTime', y='SYN_count_sum')\n",
    "    plt.title(\"SYN Packet Count Distribution During SYN Flood Attack /n +SYN_Flood\")\n",
    "    plt.ylabel(\"SYN Flag Packet Count\")\n",
    "    plt.show()'''\n",
    "\n",
    "    # Plot SYN vs ACK Flow Packets Over Time\n",
    "    # SYN vs ACK normalized\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    # sns.lineplot(data=reflection_attacks, x='HourTime', y='Bwd_sum', marker='o', label='SYN flag sum - SYN Flood Attack')\n",
    "    # sns.lineplot(data=reflection_attacks, x='HourTime', y='Fwd_sum', marker='o', label='ACK flag sum - SYN Flood Attack')\n",
    "    sns.lineplot(data=df, x='HourTime', y='Normalized_SYN_count_sum', marker='o', label='SYN flag sum - All comunication')\n",
    "    sns.lineplot(data=df, x='HourTime', y='Normalized_ACK_count_sum', marker='o', label='ACK flag sum - All comunication')\n",
    "    plt.title(\"Normalized SYN Flag Sum vs ACK Flag Sum Over Time During SYN Flood Attack\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"SYN and ACK flags\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    # SYN vs ACK\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    # sns.lineplot(data=reflection_attacks, x='HourTime', y='Bwd_sum', marker='o', label='SYN flag sum - SYN Flood Attack')\n",
    "    # sns.lineplot(data=reflection_attacks, x='HourTime', y='Fwd_sum', marker='o', label='ACK flag sum - SYN Flood Attack')\n",
    "    sns.lineplot(data=df, x='HourTime', y='SYN_count_sum', marker='o', label='SYN flag sum - All comunication')\n",
    "    sns.lineplot(data=df, x='HourTime', y='ACK_count_sum', marker='o', label='ACK flag sum - All comunication')\n",
    "    plt.title(\"SYN Flag Sum vs ACK Flag Sum Over Time During SYN Flood Attack\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"SYN and ACK flags\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Define colors with Alpha channel integrated\n",
    "    color_syn = (0.5, 0, 0.5, 0.8)  # Purple with 80% opacity\n",
    "    color_ack = (1, 0.5, 0, 0.5)  # Green with 30% opacity\n",
    "\n",
    "    # Create box plots with these colors and line settings\n",
    "    sns.boxplot(data=dos_attacks_detected_group, x='HourTime', y='SYN_count_sum', color=color_syn, linecolor=color_syn,\n",
    "                linewidth=3)\n",
    "    sns.boxplot(data=dos_attacks_detected_group, x='HourTime', y='ACK_count_sum', color=color_ack, linecolor=color_ack,\n",
    "                linewidth=3)\n",
    "\n",
    "    plt.title(\"SYN vs ACK Flags Distribution During SYN Flood Attacks\")\n",
    "    plt.ylabel(\"SYN / ACK Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(data=dos_attacks_detected_group, x='SYN_count_sum', y='ACK_count_sum', color=\"purple\", alpha=0.3)\n",
    "    plt.xlabel(\"SYN Frequency\")\n",
    "    plt.ylabel(\"ACK Frequency\")\n",
    "    plt.title(\"SYN and ACK Flags correlation During SYN Flood Attacks\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    '''plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data = df, x=df.index, y='SYN_count_sum', marker='o', label='Data')\n",
    "\n",
    "    outlier_info = detect_outliers_iqr(df, \"SYN_count_sum\")\n",
    "    upper_bound = outlier_info[\"upper_bound\"]\n",
    "    lower_bound = outlier_info[\"lower_bound\"]\n",
    "    plt.axhline(y=upper_bound, color='red', linestyle='--', label='Upper Bound (95th Percentile)')\n",
    "    plt.axhline(y=lower_bound, color='blue', linestyle='--', label='Lower Bound (5th Percentile)')\n",
    "\n",
    "    plt.title(\"SYN Count Over Time with Outlier Thresholds\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(\"SYN_count_sum\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()'''\n",
    "\n"
   ],
   "id": "1d71a49ff8253d9d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.092305Z",
     "start_time": "2025-02-09T00:19:31.079867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ACK_Flood(df_of_attack, df):\n",
    "    # Filter data for ACk Flood Attack for graphs\n",
    "    ACK_Flood = df_of_attack[df_of_attack['attack_type'] == 'ACK_Flood']\n",
    "    # Ensure 'HourTime' is properly formatted as datetime for sorting and plotting\n",
    "    if not np.issubdtype(df['HourTime'].dtype, np.datetime64):\n",
    "        df['HourTime'] = df['Datetime'].dt.floor('30min').dt.strftime('%H:%M')\n",
    "    # Sort Reflection Attack data by 'HourTime'\n",
    "    ACK_Flood = ACK_Flood.sort_values(by='Dst IP')\n",
    "    ACK_Flood = ACK_Flood.sort_values(by='HourTime')\n",
    "\n",
    "    # Plot Flow_Packets_s_avg over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=ACK_Flood, x='HourTime', y='Flow_Packets_s_avg', marker='o')\n",
    "    plt.title(\"Flow Packets Per Hour During ACK Flood Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Packets/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Flow_Bytes_s_avg over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=ACK_Flood, x='HourTime', y='Flow_Bytes_s_avg', marker='o')\n",
    "    plt.title(\"Flow Bytes Per Hour During ACK Flood Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Flow Byte/s (Average)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = ACK_Flood.pivot_table(index='HourTime', values='CountRequests', aggfunc='sum', fill_value=0)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(heatmap_data, cmap=\"YlGnBu\", annot=True, fmt=\"d\", linewidths=0.5)\n",
    "    plt.title(\"Heatmap of Request Counts During ACK Flood Attacks\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Request Count\")\n",
    "    plt.show()\n",
    "\n",
    "    # Bar chart showing CountRequests over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=ACK_Flood, x='HourTime', y='CountRequests', errorbar=None)\n",
    "    plt.title(\"ACK Flood Attack Count Over Time\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Count of Requests\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Bar chart showing ACK_count_sum over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=ACK_Flood, x='HourTime', y='ACK_count_sum', errorbar=None)\n",
    "    plt.title(\"ACK Flood ACK Count Over Time\")\n",
    "    plt.xlabel(\"Time (Hour:Minute)\")\n",
    "    plt.ylabel(\"Count of ACK Flags\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Histogram for ACK_SYN_Ratio\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(ACK_Flood['ACK_SYN_Ratio'], bins=20, kde=True, color='blue')\n",
    "    plt.axvline(ACK_Flood['ACK_SYN_Ratio'].quantile(0.75), color='red', linestyle='--', label=\"Upper Threshold\")\n",
    "    plt.axvline(ACK_Flood['ACK_SYN_Ratio'].quantile(0.25), color='green', linestyle='--', label=\"Lower Threshold\")\n",
    "    plt.title(\"Histogram of ACK_SYN_Ratio During ACK Flood Attacks\")\n",
    "    plt.xlabel(\"ACK_SYN_Ratio\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Violin plot for ACK_count_sum\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.violinplot(data=ACK_Flood, y='ACK_count_sum')\n",
    "    plt.title(\"ACK Flag Packet Count Distribution During ACK Flood Attacks\")\n",
    "    plt.ylabel(\"ACK Flag Packet Count\")\n",
    "    plt.show()\n",
    "\n",
    "    # Violin plot for SYN_count_sum\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.violinplot(data=ACK_Flood, y='SYN_count_sum')\n",
    "    plt.title(\"SYN Packet Count Distribution During ACK Flood Attacks\")\n",
    "    plt.ylabel(\"SYN Flag Packet Count\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Define colors with Alpha channel integrated\n",
    "    color_syn = (0.5, 0, 0.5, 0.5)  # Purple with 30% opacity\n",
    "    color_ack = (1, 1, 0, 0.5)  # Yellow with 30% opacity\n",
    "\n",
    "    # Create box plots with these colors and line settings\n",
    "    sns.boxplot(data=dos_attacks_detected_group, x='HourTime', y='SYN_count_sum', color=color_syn, linecolor=\"orange\",\n",
    "                linewidth=3)\n",
    "    sns.boxplot(data=dos_attacks_detected_group, x='HourTime', y='ACK_count_sum', color=color_ack, linecolor=\"purple\",\n",
    "                linewidth=3)\n",
    "\n",
    "    plt.title(\"SYN vs ACK Flags Distribution During ACK Flood Attacks\")\n",
    "    plt.ylabel(\"SYN / ACK Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(data=dos_attacks_detected_group, x='SYN_count_sum', y='ACK_count_sum', color=\"purple\", alpha=0.3)\n",
    "    plt.xlabel(\"SYN Frequency\")\n",
    "    plt.ylabel(\"ACK Frequency\")\n",
    "    plt.title(\"SYN and ACK Flags correlation During ACK Flood Attacks\")\n",
    "    plt.show()"
   ],
   "id": "8c44e51112f295a5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.098503Z",
     "start_time": "2025-02-09T00:19:31.093798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_DoS_attacks(df, normal_data=None):\n",
    "    groupData = group_data(df)\n",
    "    groupNormData = None\n",
    "    if not normal_data.empty:\n",
    "        groupNormData = group_data(normal_data)\n",
    "    # Iterating over groups to detect potential DoS attacks and classify attacks\n",
    "    dos_attacks = {}\n",
    "    for index, row in groupData.iterrows():\n",
    "        classify_attack(row, df._filename, dos_attacks, index, groupNormData)\n",
    "\n",
    "    return dos_attacks, groupData, groupNormData"
   ],
   "id": "e8eb9c4e63a3cca0",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.106092Z",
     "start_time": "2025-02-09T00:19:31.100449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_line_data(data, data2=None, x=None, y=None, hue=None, title=\"\", xlabel=\"\", ylabel=\"\", rotation=45,\n",
    "                   figsize=(12, 6), color='skyblue'):\n",
    "    flag = False\n",
    "    if isinstance(data, pd.Series):\n",
    "        data.value_counts().plot(kind='bar', color=color)\n",
    "    elif isinstance(data, pd.DataFrame) and 'x' in data.columns and 'y' in data.columns:\n",
    "        if data2 is not None:\n",
    "            if isinstance(data2, type(data)):\n",
    "                sns.lineplot(data=data2, x=x, y=y, hue=hue, marker='o')\n",
    "            sns.lineplot(data=data, x='x', y='y', color=color, marker='o')\n",
    "    if flag:\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Error: Data format not supported or incorrect parameters.\")"
   ],
   "id": "565aca59040eede9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.111784Z",
     "start_time": "2025-02-09T00:19:31.108027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_bar_data(data, title, xlabel, ylabel, color='skyblue'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data.plot(kind='bar', color=color)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ],
   "id": "406f3d63c314e433",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.123689Z",
     "start_time": "2025-02-09T00:19:31.114037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def perform_ports_protocols_graphs_fromGroupData(df, DosAttacks=None):\n",
    "    if 'SrcPort_categorical' in df.columns and 'Protocol_categorical' in df.columns:\n",
    "        port_category_counts = df[['Well_Known_Port_Count', 'Registered_Port_Count', 'Dynamic_Private_Port_Count']].sum()\n",
    "        protocol_category_counts = df[['Protocol_TCP_Count', 'Protocol_UDP_Count', 'Protocol_ICMP_Count', 'Other_Protocol_Count']].sum()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        port_category_counts.plot(kind='bar', color=['blue', 'orange', 'green'])\n",
    "        plt.title(\"Distribution of Port Categories\")\n",
    "        plt.xlabel(\"Port Category\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        protocol_category_counts.plot(kind='bar', color=['red', 'purple', 'cyan', 'pink'])\n",
    "        plt.title(\"Distribution of Protocol Categories\")\n",
    "        plt.xlabel(\"Protocol Category\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "        if DosAttacks is not None and 'SrcPort_categorical' in DosAttacks.columns and 'Protocol_categorical' in DosAttacks.columns:\n",
    "            attack_port_counts = DosAttacks[['Well_Known_Port_Count', 'Registered_Port_Count', 'Dynamic_Private_Port_Count']].sum()\n",
    "            attack_protocol_counts = DosAttacks[['Protocol_TCP_Count', 'Protocol_UDP_Count', 'Protocol_ICMP_Count', 'Other_Protocol_Count']].sum()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            attack_port_counts.plot(kind='bar', color=['blue', 'orange', 'green'])\n",
    "            plt.title(\"Port Category Distribution in Detected Attacks\")\n",
    "            plt.xlabel(\"Port Category\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            attack_protocol_counts.plot(kind='bar', color=['red', 'purple', 'cyan', 'pink'])\n",
    "            plt.title(\"Protocol Category Distribution in Detected Attacks\")\n",
    "            plt.xlabel(\"Protocol Category\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"Required columns are missing from DataFrame.\")"
   ],
   "id": "1d7395c20298fda7",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.130691Z",
     "start_time": "2025-02-09T00:19:31.125320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def attack_det(df_of_attack, str_type = None):\n",
    "    \"\"\"\n",
    "    Analyze and visualize Attacks.\n",
    "\n",
    "    This function focuses on classified attacks and provides detailed descriptions and visualizations of the unique sources,\n",
    "    their ports, and protocols.\n",
    "    \"\"\"\n",
    "    # Format 'HourTime' as a datetime column for sorting\n",
    "    if not np.issubdtype(df_of_attack['HourTime'].dtype, np.datetime64):\n",
    "        df_of_attack['HourTime'] = df_of_attack['Datetime'].dt.floor('30min').dt.strftime('%H:%M')\n",
    "    # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "    df_of_attack_type = df_of_attack.sort_values(by='Dst IP')\n",
    "    df_of_attack_type = df_of_attack_type.sort_values(by='HourTime')\n",
    "    if str_type is not None:\n",
    "        # Filter data for Attack Type as in the input\n",
    "         if str_type is str:\n",
    "            df_of_attack_type = df_of_attack_type[df_of_attack_type['attack_type'] == str_type]\n",
    "    print(f\"{str_type} Detected:\") \n",
    "    # Iterate over each Attack for detailed analysis\n",
    "    for index, attack in df_of_attack_type.iterrows():\n",
    "            print(f\"Attack ID: {index}\")\n",
    "            describe_df(attack)\n",
    "    return df_of_attack_type  "
   ],
   "id": "3e79322013a09d0",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:31.140929Z",
     "start_time": "2025-02-09T00:19:31.133292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def perform_attacks(df, normal_data=None):\n",
    "    # how many of each attack:\n",
    "    n_SYN_Flood = 0\n",
    "    n_Reflection_Attack = 0\n",
    "    n_ACK_Flood = 0\n",
    "    detected_attacks_dict, detected_group, groupNormData = detect_DoS_attacks(df, normal_data)\n",
    "\n",
    "    # Display descriptive statistics\n",
    "    print(\"Grouped Data Description:\")\n",
    "    describe_df(detected_group)\n",
    "    \n",
    "    if groupNormData is not None:\n",
    "        if not groupNormData.empty:\n",
    "            # Display descriptive statistics\n",
    "            print(\"Normal Data to Compare to:\")\n",
    "            print(\"Grouped Normal Data Description:\")\n",
    "            describe_df(groupNormData)\n",
    "            \n",
    "    for attack in detected_attacks_dict.values():\n",
    "        if attack['attack_type'] == 'SYN_Flood':\n",
    "            n_SYN_Flood += 1\n",
    "        elif attack['attack_type'] == 'Reflection_Attack':\n",
    "            n_Reflection_Attack += 1\n",
    "        elif attack['attack_type'] == 'ACK_Flood':\n",
    "            n_ACK_Flood += 1\n",
    "    print(f\"SYN_Flood: {n_SYN_Flood}\\nReflection_Attack: {n_Reflection_Attack}\\nACK_Flood: {n_ACK_Flood}\")\n",
    "    print()\n",
    "\n",
    "    if detected_attacks_dict is None:\n",
    "        print(\"No DoS attack has been detected.\")\n",
    "        return\n",
    "\n",
    "    # To perform a statistic description of a dictionary, we need to convert it into a pd Series or DataFrame:\n",
    "    # Convert to DataFrame\n",
    "    attacks_df = pd.DataFrame.from_dict(detected_attacks_dict, orient='index')\n",
    "    # Display basic statistics\n",
    "    describe_df(attacks_df)\n",
    "    print(attacks_df.head())\n",
    "\n",
    "    #    print(\n",
    "    #        f\"\\nAttacks details in original grouped df:\\n{detected_group[detected_group.index.map(lambda x: x in attacks_df.index)]}\\n\\n\")\n",
    "\n",
    "    # Display descriptive statistics and perform analysis and plotting for each attack type\n",
    "    if n_SYN_Flood > 0:\n",
    "        SYN_Flood(attack_det(attacks_df, \"SYN_Flood\"), detected_group)\n",
    "    if n_Reflection_Attack > 0:\n",
    "        Reflection_Attack(attack_det(attacks_df,\"Reflection_Attack\"), detected_group)\n",
    "    if n_ACK_Flood > 0:\n",
    "        ACK_Flood(attack_det(attacks_df,\"ACK_Flood\"), detected_group)\n",
    "    if n_SYN_Flood == n_Reflection_Attack == n_ACK_Flood == 0:\n",
    "        print(\"No DoS attacks have been recognized.\")\n",
    "   \n",
    "    return detected_attacks_dict, detected_group, groupNormData"
   ],
   "id": "1692ae6211ecce90",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dos_attacks_dict, dos_attacks_detected_group, normal_data_group = perform_attacks(df_tcpdump_friday, df_pvt_monday)",
   "id": "deaa332e7a74bb83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:19:40.510688Z",
     "start_time": "2025-02-09T00:19:40.510197Z"
    }
   },
   "cell_type": "code",
   "source": "graphs_for_outliers(dos_attacks_detected_group, normal_data_group,pd.DataFrame.from_dict(dos_attacks_dict, orient='index'))",
   "id": "de7b24fc6be3dfa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:26:23.411904Z",
     "start_time": "2025-02-09T00:26:23.398681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "perform_ports_protocols_graphs_fromGroupData(dos_attacks_detected_group,\n",
    "                                             pd.DataFrame.from_dict(dos_attacks_dict, orient='index'))"
   ],
   "id": "b3d1b4a80f7646a8",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dos_attacks_detected_group' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m perform_ports_protocols_graphs_fromGroupData(\u001B[43mdos_attacks_detected_group\u001B[49m,\n\u001B[1;32m      2\u001B[0m                                              pd\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39mfrom_dict(dos_attacks_dict, orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dos_attacks_detected_group' is not defined"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T00:25:22.390886Z",
     "start_time": "2025-02-09T00:25:22.376541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extra check \n",
    "max_syn_count = dos_attacks_detected_group['SYN_count_sum'].max()\n",
    "print(f\"Max SYN Count: {max_syn_count}\")\n",
    "print(dos_attacks_detected_group[dos_attacks_detected_group['SYN_count_sum'] == max_syn_count])"
   ],
   "id": "26e6040bc363f117",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dos_attacks_detected_group' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# extra check \u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m max_syn_count \u001B[38;5;241m=\u001B[39m \u001B[43mdos_attacks_detected_group\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSYN_count_sum\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmax()\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMax SYN Count: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_syn_count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(dos_attacks_detected_group[dos_attacks_detected_group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSYN_count_sum\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m max_syn_count])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dos_attacks_detected_group' is not defined"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T02:20:33.588640Z",
     "start_time": "2025-02-07T02:20:33.578990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def graph_for_outlierCol_seperated(df, outlier_dict, normData=None, outlierNormal_dict=None, attack=None):\n",
    "    column = outlier_dict['column']\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column in df.columns:\n",
    "        # Calculate the necessary statistics for the column\n",
    "        col_mid = df[column].median()\n",
    "        # col_std = df[column].std()\n",
    "        lower_bound = outlier_dict['lower_bound']\n",
    "        upper_bound = outlier_dict['upper_bound']\n",
    "\n",
    "        # Ensure 'HourTime' is properly formatted as datetime for sorting and plotting\n",
    "        if not np.issubdtype(df['HourTime'].dtype, np.datetime64):\n",
    "            df['HourTime'] = df['Datetime'].dt.floor('30min').dt.strftime('%H:%M')\n",
    "\n",
    "        # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "        df = df.sort_values(by='Dst IP')\n",
    "        df = df.sort_values(by='HourTime')\n",
    "\n",
    "        # Plot the data with horizontal lines for thresholds\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.lineplot(data=df, x='HourTime', y=column, marker='o', color='y', label='Analysed Data')\n",
    "        # Add horizontal lines to indicate statistical thresholds\n",
    "        plt.axhline(y=col_mid, color='blue', linestyle='dashed', linewidth=2, label='Median')\n",
    "        # plt.axhline(y=col_mid - col_std, color='sky blue', linestyle='dashed', linewidth=2, label='-1 STD')\n",
    "        # plt.axhline(y=col_mid + col_std, color='sky blue', linestyle='dashed', linewidth=2, label='+1 STD')\n",
    "        plt.axhline(y=lower_bound, color='y', linestyle='dashed', linewidth=2, label='Lower Bound')\n",
    "        plt.axhline(y=upper_bound, color='y', linestyle='dashed', linewidth=2, label='Upper Bound')\n",
    "        # Add titles, labels, and legend for clarity\n",
    "        plt.title(f\"{column} Over Time with Horizontal Thresholds\", fontsize=14,\n",
    "                  fontweight='bold')\n",
    "        plt.xlabel(\"Time (Hour:Minute)\", fontsize=12)\n",
    "        plt.ylabel(f\"{column} Values\", fontsize=12)\n",
    "        plt.xticks(df['HourTime'].unique(), rotation=45)\n",
    "        plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        # Normal data\n",
    "        if not normData.empty:\n",
    "            # Ensure the column exists in the DataFrame\n",
    "            if column in normData.columns:\n",
    "                # Calculate the necessary statistics for the column\n",
    "                n_col_mid = normData[column].median()\n",
    "                # n_col_std = normData[nColumn].std()\n",
    "                lower_nBound = outlierNormal_dict['lower_bound']\n",
    "                upper_nBound = outlierNormal_dict['upper_bound']\n",
    "                # Ensure 'HourTime' is properly formatted as datetime for sorting and plotting\n",
    "                if not np.issubdtype(normData['HourTime'].dtype, np.datetime64):\n",
    "                    normData['HourTime'] = normData['Datetime'].dt.floor('15T').dt.strftime('%H:%M')\n",
    "                # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "                normData = normData.sort_values(by='Dst IP')\n",
    "                normData = normData.sort_values(by='HourTime')\n",
    "                sns.lineplot(data=normData, x='HourTime', y=column, marker='o', color='c', label='Normal Data')\n",
    "                # Add horizontal lines to indicate statistical thresholds\n",
    "                plt.axhline(y=n_col_mid, color='g', linestyle='dashed', linewidth=2, label='Normal Median')\n",
    "                # plt.axhline(y=n_col_mid - n_col_std, color='gray', linestyle='dashed', linewidth=2, label='-1 Normal STD')\n",
    "                # plt.axhline(y=n_col_mid + n_col_std, color='gray', linestyle='dashed', linewidth=2, label='+1 Normal STD')\n",
    "                plt.axhline(y=lower_nBound, color='r', linestyle='dashed', linewidth=2, label='Lower Normal Bound')\n",
    "                plt.axhline(y=upper_nBound, color='r', linestyle='dashed', linewidth=2, label='Upper Normal Bound')\n",
    "                # Add titles, labels, and legend for clarity\n",
    "                plt.title(f\"{column} Over Time with Horizontal Thresholds:\\nNormal Data vs Compared Data\", fontsize=14,\n",
    "                          fontweight='bold')\n",
    "                plt.xlabel(\"Time (Hour:Minute)\", fontsize=12)\n",
    "                plt.ylabel(f\"{column} Values\", fontsize=12)\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        # --- Notes for Horizontal Lines ---\n",
    "        # Horizontal lines highlight statistical thresholds (median, ±std, and bounds) on the y-axis.\n",
    "        # This is useful to identify patterns or anomalies in the column's value distribution over time.\n",
    "\n",
    "        # Plot the data with vertical lines for specific thresholds (time-based visualization)\n",
    "\n",
    "        # Add vertical lines for thresholds\n",
    "\n",
    "        # --- Notes for Vertical Lines ---\n",
    "        # Vertical lines are used to highlight specific time points (e.g., first and last recorded times).\n",
    "        # These are useful for marking events or key time intervals on the x-axis.\n",
    "\n",
    "    else:\n",
    "        return None"
   ],
   "id": "7c19271ee319020b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T02:20:33.829028Z",
     "start_time": "2025-02-07T02:20:33.590884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_for_outlierCol_seperated(dos_attacks_detected_group, normal_data_group,\n",
    "                   attack_det(pd.DataFrame.from_dict(dos_attacks_dict, orient='index')))\n"
   ],
   "id": "552a96f80a7678ed",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graphs_for_outliers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mgraphs_for_outliers\u001B[49m(dos_attacks_detected_group, normal_data_group,\n\u001B[1;32m      2\u001B[0m                     SYN_Flood(pd\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39mfrom_dict(dos_attacks_dict, orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'graphs_for_outliers' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T12:54:50.772002Z",
     "start_time": "2025-02-05T12:54:50.765404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def boxplot_for_outlierCol(df, column, normData=None):\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column in df.columns:\n",
    "        # Ensure 'HourTime' is properly formatted as datetime for sorting and plotting\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['HourTime']):\n",
    "            df['HourTime'] = pd.to_datetime(df['Datetime']).dt.floor('30min').dt.strftime('%H:%M')\n",
    "        # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "        df = df.sort_values(by='Dst IP')\n",
    "        df = df.sort_values(by='HourTime')\n",
    "\n",
    "        #Boxplot\n",
    "        plt.figure(figsize=(18, 10))\n",
    "        sns.boxplot(data=df, x='HourTime', y=column, color=\"purple\", linecolor=\"purple\",linewidth=3, label = \"Analysed Data\")\n",
    "        # Add titles, labels, and legend for clarity\n",
    "        plt.title(f\"{column} Distribution Over Time\")\n",
    "        plt.xlabel(\"Time (Hour:Minute)\")\n",
    "        plt.ylabel(f\"{column} Values\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        # Normal data\n",
    "        if not normData.empty:\n",
    "            # Ensure the column exists in the DataFrame\n",
    "            if column in normData.columns:\n",
    "                flag = True\n",
    "                # Ensure 'HourTime' is properly formatted as datetime for sorting and plotting\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df['HourTime']):\n",
    "                    normData['HourTime'] = pd.to_datetime(normData['Datetime']).dt.floor('30min').dt.strftime('%H:%M')\n",
    "                # Sort grouped data by 'Dst IP' and 'HourTime' to ensure the correct order\n",
    "                normData = normData.sort_values(by='Dst IP')\n",
    "                normData = normData.sort_values(by='HourTime')\n",
    "\n",
    "                #Boxplot\n",
    "                sns.boxplot(data=df, x='HourTime', y=column, color=\"purple\", linecolor=\"purple\", linewidth=3, label = \"Compared Data\")\n",
    "                sns.boxplot(data=normData, x='HourTime', y=column, color=\"orange\", linecolor=\"orange\", linewidth=3, label = \"Normal Data\")\n",
    "                # Add titles, labels, and legend for clarity\n",
    "                plt.title(f\"{column} Distribution Over Time\")\n",
    "                plt.xlabel(\"Time (Hour:Minute)\")\n",
    "                plt.ylabel(f\"{column} Values\")\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "                plt.show()\n",
    "\n",
    "    else:\n",
    "        return None\n"
   ],
   "id": "655f85a8e3b4f3f4",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T16:31:58.052939Z",
     "start_time": "2025-02-01T16:31:58.048116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def boxGraphs_for_outliers(df, normal_data=None):\n",
    "    flag = True\n",
    "    zscore_N_dict = None\n",
    "    outlier_N_dict = None\n",
    "    for col in df.columns:\n",
    "        for _, row in df.iterrows():\n",
    "            if (row.get(f'Outlier_Normalized_{col}_U/L') or row.get(f'Outlier_{col}_U/L')) and flag:\n",
    "                # Transform the dictionaries into DataFrames\n",
    "                zscore_dict = detect_zscore_outliers_iqr(df, col)\n",
    "                outlier_dict = detect_outliers_iqr(df, col)\n",
    "                df1 = pd.DataFrame.from_dict(zscore_dict, orient='index', columns=['zscore_values'])\n",
    "                df2 = pd.DataFrame.from_dict(outlier_dict, orient='index', columns=['outlier_values'])\n",
    "                # Combine the DataFrames (e.g., vertically or horizontally)\n",
    "                # Horizontally: Adding columns\n",
    "                combined_df_horizontal = pd.concat([df1, df2], axis=1)\n",
    "                if normal_data.empty:\n",
    "                    print(combined_df_horizontal)\n",
    "                    flag = False\n",
    "                else:\n",
    "                    if row[col] > detect_zscore_outliers_iqr(normal_data, col)['upper_bound'] or row[\n",
    "                        f\"Normalized_{col}\"] > \\\n",
    "                            detect_outliers_iqr(normal_data, col)['upper_bound'] or row[col] < \\\n",
    "                            detect_zscore_outliers_iqr(normal_data, col)['lower_bound'] or row[f\"Normalized_{col}\"] < \\\n",
    "                            detect_outliers_iqr(normal_data, col)['lower_bound']:\n",
    "                        print(combined_df_horizontal)\n",
    "                        zscore_N_dict = detect_zscore_outliers_iqr(df, col)\n",
    "                        outlier_N_dict = detect_outliers_iqr(df, col)\n",
    "                        flag = False\n",
    "                boxplot_for_outlierCol(df, col, normal_data)\n",
    "\n",
    "        if not flag:\n",
    "            print(f\"No Outlier values for {col} in this DF\")\n",
    "        flag = True\n"
   ],
   "id": "7d66c2d3d883f16d",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "boxGraphs_for_outliers(dos_attacks_detected_group, normal_data_group)\n",
   "id": "c16cb50b31b71ff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "86073b85c34dd250"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
